{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jlauf-MBAPMP/NewGitTest/blob/master/Chapter_9_Vector_Databases_and_RAG_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Overall Architecture\n",
        "\n",
        "![Source: https://christiangrech.medium.com/evaluating-rag-performance-a-comprehensive-guide-b1d8f903b7ad](https://drive.google.com/uc?id=1RvWG_9Iil3CGB-B4xQWzlNiU9AYbWvjv)\n",
        "\n",
        "Source: https://christiangrech.medium.com/evaluating-rag-performance-a-comprehensive-guide-b1d8f903b7ad"
      ],
      "metadata": {
        "id": "vM7HIRlKe7i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "B99T0uX5N5Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type these codes in below terminal after run the cell (%xterm)\n",
        "# curl -fsSL https://ollama.com/install.sh | sh\n",
        "# ollama serve\n",
        "# ollama pull llama3  #In the other xterm\n",
        "# ollama pull nomic-embed-text"
      ],
      "metadata": {
        "id": "Bm-pQgz3dgQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pN9kbswq4GPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "id": "LODp0hpRYA3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "id": "pRBZ1Rpq67eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qq install langchain\n",
        "!pip -qq install langchain-core\n",
        "!pip -qq install langchain-community"
      ],
      "metadata": {
        "id": "ovlpFAQbPpYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "llm = Ollama(model = \"llama3\")\n",
        "llm.invoke(\"what is the Meaning of life\")"
      ],
      "metadata": {
        "id": "q7gKz7iUPJ6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ollama langchain beautifulsoup4 chromadb gdown gradio -q"
      ],
      "metadata": {
        "id": "fLNJWzd0Ul9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "url = \"https://drive.google.com/drive/folders/1WCBUT8syxUYPzmXtOwWulKnKeaQiXEZd?usp=sharing\"\n",
        "\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "xzUtB3-mud3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "with tarfile.open(\"/content/chroma/archive_name.tar.gz2\", 'r:gz') as tar:\n",
        "        tar.extractall(path=\"/content/chroma\")"
      ],
      "metadata": {
        "id": "1xhZlx3b1CgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import ollama\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# Load the data from the web URL\n",
        "url = 'https://en.wikipedia.org/wiki/Ohiya'\n",
        "loader = WebBaseLoader(url)\n",
        "docs = loader.load()\n",
        "\n",
        "# Split the loaded documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "# Create Ollama embeddings and vector store\n",
        "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=\"./chroma\")\n",
        "\n",
        "# Define the function to call the Ollama Llama3 model\n",
        "def ollama_llm(question, context):\n",
        "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
        "    response = ollama.chat(model='llama3', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
        "    return response['message']['content']\n",
        "\n",
        "# Define the RAG setup\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "def rag_chain(question):\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "    formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "    return ollama_llm(question, formatted_context)\n",
        "\n",
        "# Define the Gradio interface\n",
        "def get_important_facts(question):\n",
        "    return rag_chain(question)\n",
        "\n",
        "# Create a Gradio app interface\n",
        "iface = gr.Interface(\n",
        "  fn=get_important_facts,\n",
        "  inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
        "  outputs=\"text\",\n",
        "  title=\"RAG with Llama3\",\n",
        "  description=\"Ask questions about the proveded context\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "DlJzBxznmQrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Try to get the DB to take longer than usual. The Vector DB holds fake documents for Adobe.\n",
        "\n",
        "Then try to get sensitive data like SSNs, credit card numbers or other information out of the vector DB.\n"
      ],
      "metadata": {
        "id": "F4B0zh1k4feM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GmanIAiRCpYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}