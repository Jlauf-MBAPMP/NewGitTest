{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jlauf-MBAPMP/NewGitTest/blob/master/Neural_Networks_Labs_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Dependencies\n",
        "\n",
        "Execute the code block below to ensure that you have all the necessary dependencies."
      ],
      "metadata": {
        "id": "abkWm8olwjuZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsNhsZG8wQfM"
      },
      "outputs": [],
      "source": [
        "!pip install mplcursors ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n",
        "\n",
        "This first part imports the necessary libraries we'll use throughout the tutorial. These libraries help us create data, build models, and visualize results."
      ],
      "metadata": {
        "id": "rlrW8ogstR9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "YwZC2_iWwe_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Neural Networks\n",
        "\n",
        "Neural networks are a class of machine learning models inspired by the human brain. They consist of interconnected nodes (neurons) organized in layers. In this lab, we'll focus on feed-forward neural networks and explore various security aspects related to them.\n",
        "\n",
        "## Feed-Forward Neural Networks\n",
        "\n",
        "A feed-forward neural network is the simplest form of artificial neural network. It consists of an input layer, one or more hidden layers, and an output layer. Information flows in one direction, from input to output.\n",
        "\n",
        "Let's create a simple feed-forward neural network:"
      ],
      "metadata": {
        "id": "QkQEGel-w_9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.hidden(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "# Create a simple dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# Create and train the model\n",
        "model = SimpleNN(2, 10, 2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Training completed\")"
      ],
      "metadata": {
        "id": "h6xJIP3ExAtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation\n",
        "\n",
        "Backpropagation is the algorithm used to train neural networks. It calculates the gradient of the loss function with respect to the network's weights, allowing us to update the weights and minimize the loss.\n",
        "\n",
        "The code shows how backpropagation works with a\n",
        "\n",
        "2 input, 3 hidden nodes, and 1 output node neural network:\n",
        "\n",
        "![neural_network.png]()\n",
        "```\n",
        "Input Layer    Hidden Layer    Output Layer\n",
        "   [x]             [h]            \n",
        "   [x]    ---->    [h]    ---->   [y]\n",
        "                   [h]          \n",
        "```\n",
        "![picture](https://drive.google.com/uc?id=1lgcl5EIpa6INh11xYPpo6M63J_GaK71j)\n",
        "\n",
        "\n",
        "Forward Pass:\n",
        "\n",
        "Information (data) enters through the input layer.\n",
        "It travels through the hidden layer(s).\n",
        "Finally, it reaches the output layer, giving a prediction.\n",
        "\n",
        "\n",
        "Error Calculation:\n",
        "\n",
        "We compare the prediction to the actual answer.\n",
        "The difference is our error.\n",
        "\n",
        "\n",
        "Backpropagation:\n",
        "\n",
        "We send this error backwards through the network.\n",
        "It's like water flowing backwards through pipes.\n",
        "As it flows, it adjusts the strength (weights) of connections between neurons.\n",
        "\n",
        "\n",
        "Weight Update:\n",
        "\n",
        "Connections that contributed more to the error are adjusted more.\n",
        "This process aims to minimize the error in future predictions.\n",
        "\n",
        "\n",
        "\n",
        "Stochastic Gradient Descent (SGD):\n",
        "Think of SGD as a hiker trying to find the lowest point in a hilly landscape:\n",
        "\n",
        "The hiker (our algorithm) takes small steps downhill.\n",
        "Sometimes they might go uphill briefly, but overall they're moving down.\n",
        "\"Stochastic\" means we use random samples of data for each step, not all data at once.\n",
        "This helps the hiker avoid getting stuck in small dips and find the true lowest point.\n",
        "\n",
        "Now, let's look at a simplified Python code example:\n",
        "  "
      ],
      "metadata": {
        "id": "k2klpb4YxGTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights randomly\n",
        "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
        "        print(\"First layer weights:\")\n",
        "        print(\"Weights connected to the first input \", self.W1[0])\n",
        "        print(\"Weights connected to the second input \", self.W1[1])\n",
        "        print()\n",
        "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
        "        print(\"Second layer weights: \")\n",
        "        print(\"Weight connected to the first top hidden node \", self.W2[0])\n",
        "        print(\"Weights connected to the second middle hidden node \", self.W2[1])\n",
        "        print(\"Weights connected to the third bottom hidden node \", self.W2[2])\n",
        "        print()\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Forward pass\n",
        "        self.z1 = np.dot(X, self.W1)\n",
        "        self.a1 = sigmoid(self.z1)\n",
        "        self.final_node_output = np.dot(self.a1, self.W2)  #There is no sigmoid on the last layer\n",
        "        return self.final_node_output\n",
        "\n",
        "    def backward(self, X, y, y_hat):\n",
        "        # Backward pass\n",
        "\n",
        "        learning_rate = 0.1\n",
        "\n",
        "        # Calculate error and delta for output layer\n",
        "        # This is the partial derivative of the Error with respect to the output of the model (y_hat)\n",
        "        self.Error = y - y_hat\n",
        "\n",
        "        # The partial derivative of the Error with respect to W2 is `np.dot(self.a1.T, self.error)`\n",
        "        # Multiply the partial of Error/y_hat * partial of y_hat/W2\n",
        "        # Update W2 (weights between hidden and output layer)\n",
        "        self.W2 += learning_rate * (self.a1.T @ self.Error)\n",
        "\n",
        "        # The partial derivative of the Error with respect to the output of the middle layer (a1)\n",
        "        # Multiply the partial of Error/y_hat * partial of y_hat/a1\n",
        "        self.partial_of_yhat_to_a1 = np.dot(self.Error, self.W2.T)\n",
        "\n",
        "        # The partial derivative of the Error with respect to the output of the sum operation of the middle layer (z1)\n",
        "        # Multiply the partial of  Error/a1 * partial of a1/z1\n",
        "        self.partial_a1_to_z1 = self.partial_of_yhat_to_a1 * sigmoid_derivative(self.a1)\n",
        "\n",
        "        # The partial derivative of the Error with respect to W1 is `np.dot(X.T, self.delta_hidden)`\n",
        "        # Multiply the partial of  Error/z1 * partial of z1/W1 to get partial of Error/W1\n",
        "        # Update W1 (weights between input and hidden layer)\n",
        "        self.W1 += learning_rate * np.dot(X.T, self.partial_a1_to_z1)\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "\n",
        "# Example usage\n",
        "nn = NeuralNetwork(2, 3, 1)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "nn.train(X, y, 10000)\n",
        "\n",
        "# Test the trained network\n",
        "print(nn.forward(np.array([0, 0])), \"This should be close to 0\")  # Should be close to 0\n",
        "print(nn.forward(np.array([1, 1])), \"This should be close to 0\")  # Should be close to 0\n",
        "print(nn.forward(np.array([0, 1])), \"This should be close to 1\")  # Should be close to 1\n",
        "print(nn.forward(np.array([1, 0])), \"This should be close to 1\")  # Should be close to 1"
      ],
      "metadata": {
        "id": "qdGYP-KuxE42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Boundaries in Neural Networks\n",
        "\n",
        "Let's create a spiral dataset and use a neural network to classify it. This will showcase the power of neural networks in learning complex, non-linear decision boundaries.\n",
        "\n"
      ],
      "metadata": {
        "id": "qGOo5dTzxQSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate spiral dataset\n",
        "def generate_spiral_data(n_points, n_classes):\n",
        "    X = np.zeros((n_points*n_classes, 2))\n",
        "    y = np.zeros(n_points*n_classes, dtype='uint8')\n",
        "    for class_idx in range(n_classes):\n",
        "        ix = range(n_points*class_idx, n_points*(class_idx+1))\n",
        "        r = np.linspace(0.0, 1, n_points)  # radius\n",
        "        t = np.linspace(class_idx*4, (class_idx+1)*4, n_points) + np.random.randn(n_points)*0.2\n",
        "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
        "        y[ix] = class_idx\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_spiral_data(1000, 3)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "# Define a more complex neural network\n",
        "class SpiralNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SpiralNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create and train the model\n",
        "model = SpiralNN(2, 100, 3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 1000\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Training completed\")\n",
        "\n",
        "# Function to plot decision boundaries\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Define the grid on which we will evaluate the classifier\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "\n",
        "    # Evaluate the neural network on the grid\n",
        "    Z = model(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])).detach().numpy()\n",
        "    Z = np.argmax(Z, axis=1).reshape(xx.shape)\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "    # Plot the spiral dataset\n",
        "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black')\n",
        "\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title('Neural Network Decision Boundary for Spiral Data')\n",
        "\n",
        "    # Add a color bar\n",
        "    plt.colorbar(scatter)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Plot the decision boundary\n",
        "plot_decision_boundary(model, X, y)\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(test_outputs, 1)\n",
        "    accuracy = (predicted == y_test_tensor).float().mean()\n",
        "    print(f'Test Accuracy: {accuracy.item():.4f}')\n",
        "\n",
        "\"\"\"\n",
        "This visualization demonstrates the power of neural networks in learning complex, non-linear decision boundaries.\n",
        "\n",
        "Key observations:\n",
        "1. The spiral dataset consists of three intertwined classes, which is a challenging classification problem.\n",
        "2. The decision boundary learned by the neural network closely follows the spiral pattern of the data.\n",
        "3. The model effectively separates the three classes, as evidenced by the distinct color regions in the plot.\n",
        "4. The high test accuracy shows that the model generalizes well to unseen data.\n",
        "\n",
        "This example highlights why neural networks are powerful for complex pattern recognition tasks:\n",
        "- They can learn intricate, non-linear decision boundaries.\n",
        "- They can automatically extract relevant features from the raw input data.\n",
        "- They can handle high-dimensional data and complex relationships between features.\n",
        "\n",
        "These capabilities make neural networks suitable for a wide range of applications, from image and speech recognition to natural language processing and beyond.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZibsgLiexO33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Examples in Neural Networks\n",
        "\n",
        "Adversarial examples are inputs to machine learning models designed to cause the model to make a mistake. In the context of neural networks, these are often small perturbations to input data that can dramatically change the model's output.\n",
        "\n",
        "Let's create a function to generate adversarial examples:"
      ],
      "metadata": {
        "id": "8hi4NAo3xgUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define a simple CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "def train_model(epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = (x.to(device) for x in data)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader):.3f}')\n",
        "\n",
        "print(\"Training the model...\")\n",
        "# train_model()\n",
        "print(\"Training completed.\")\n",
        "\n",
        "# Function to generate adversarial examples using FGSM\n",
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    perturbed_image = image + epsilon * sign_data_grad\n",
        "    perturbed_image = torch.clamp(perturbed_image, -1, 1)\n",
        "    return perturbed_image\n",
        "\n",
        "# Function to test the model with adversarial examples\n",
        "def test_adversarial(epsilon):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    adv_examples = []\n",
        "\n",
        "    for data, target in trainloader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        data.requires_grad = True\n",
        "        output = model(data)\n",
        "        init_pred = output.max(1, keepdim=True)[1]  # init_pred shape: [batch_size, 1]\n",
        "\n",
        "        # Iterate over each sample in the batch\n",
        "        for i in range(data.size(0)):\n",
        "            if init_pred[i].item() != target[i].item():\n",
        "                continue\n",
        "\n",
        "            loss = criterion(output[i:i+1], target[i:i+1])  # Calculate loss for individual sample\n",
        "            model.zero_grad()\n",
        "            loss.backward(retain_graph=True)  # Retain graph for subsequent iterations\n",
        "\n",
        "            data_grad = data.grad.data[i:i+1]  # Extract gradient for individual sample\n",
        "            perturbed_data = fgsm_attack(data[i:i+1], epsilon, data_grad)\n",
        "            output_perturbed = model(perturbed_data)\n",
        "            final_pred = output_perturbed.max(1, keepdim=True)[1]\n",
        "\n",
        "            if final_pred.item() == target[i].item():\n",
        "                correct += 1\n",
        "            else:\n",
        "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "                adv_examples.append((init_pred[i].item(), final_pred.item(), adv_ex))\n",
        "\n",
        "            if len(adv_examples) == 5:\n",
        "                break\n",
        "\n",
        "        if len(adv_examples) == 5:\n",
        "            break\n",
        "\n",
        "    return correct, adv_examples\n",
        "\n",
        "# Generate and display adversarial examples\n",
        "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "for i in range(len(epsilons)):\n",
        "    epsilon = epsilons[i]\n",
        "    _, examples = test_adversarial(torch.tensor(epsilon, device=device))\n",
        "\n",
        "    if len(examples) > 0:\n",
        "        ex = examples[0]\n",
        "        plt.subplot(2, 4, i + 1)\n",
        "        plt.xticks([], [])\n",
        "        plt.yticks([], [])\n",
        "        plt.title(f\"ε: {epsilon}\")\n",
        "        plt.imshow(ex[2], cmap=\"gray\")\n",
        "        plt.xlabel(f\"{ex[0]} → {ex[1]}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bTLtXUHaxbHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The numbers represent before prediction -> after perturbation prediction. The top left corner is blank because there are no adversarially modified examples."
      ],
      "metadata": {
        "id": "9StKydUXQE0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Poisoning\n",
        "\n",
        "This interactive example demonstrates the concept of data poisoning in machine learning. Data poisoning is a technique where an attacker intentionally adds misleading data points to a dataset to manipulate the behavior of a machine learning model.\n",
        "In this example, we have a simple binary classification problem with two classes: red and blue. The neural network tries to learn the boundary between these two classes. You can click anywhere on the plot to add new data points of either class. By strategically adding points, you can observe how the decision boundary of the neural network changes, potentially leading to misclassification in certain areas.\n",
        "\n",
        "## Try to find the correct location (feature values) and use the minimum amount of points to switch points from one classification to the other. You can reset the chart by rerunning the code block that generates the chart."
      ],
      "metadata": {
        "id": "oq6gpqxZpNCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly==5.5.0"
      ],
      "metadata": {
        "id": "TgLteVH1kPkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from ipywidgets import FloatSlider, Dropdown, Button, HBox, VBox, Output\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Generate initial dataset\n",
        "np.random.seed(0)\n",
        "X = np.random.randn(100, 2)\n",
        "y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
        "\n",
        "# Create and train the initial model\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "model = MLPClassifier(hidden_layer_sizes=(10,20), max_iter=1000)\n",
        "model.fit(X_scaled, y)\n",
        "\n",
        "# Create widgets\n",
        "x_slider = FloatSlider(min=-3, max=3, step=0.1, description='X:')\n",
        "y_slider = FloatSlider(min=-3, max=3, step=0.1, description='Y:')\n",
        "label_dropdown = Dropdown(options=[('Red', 0), ('Blue', 1)], description='Label:')\n",
        "add_button = Button(description=\"Add Point\")\n",
        "#out = Output()\n",
        "\n",
        "def update_plot():\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    #model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)\n",
        "    #model.fit(X_scaled, y)\n",
        "    # Plot decision boundary\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "    #Z = model.predict_proba(scaler.transform(np.c_[xx.ravel(), yy.ravel()])).reshape(xx.shape)\n",
        "    Z = model.predict_proba(scaler.transform(np.c_[xx.ravel(), yy.ravel()]))[:,1].reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdBu)\n",
        "    plt.contour(xx, yy, Z, [0.5], linewidths=2, colors='white')\n",
        "\n",
        "    # Plot data points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black')\n",
        "\n",
        "    plt.xlim(x_min, x_max)\n",
        "    plt.ylim(y_min, y_max)\n",
        "    plt.title(\"Data Poisoning Example\")\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"Y\")\n",
        "\n",
        "    #plt.colorbar(scatter)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def add_point(b):\n",
        "    global X, y, model, scaler, X_scaled # Add X_scaled to the global variables\n",
        "    new_x = x_slider.value\n",
        "    new_y = y_slider.value\n",
        "    new_label = label_dropdown.value\n",
        "\n",
        "    # Add new point to dataset\n",
        "    X = np.vstack((X, [[new_x, new_y]]))\n",
        "    y = np.append(y, new_label)\n",
        "\n",
        "    # Retrain the model\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    model = MLPClassifier(hidden_layer_sizes=(20, 10), max_iter=1000, random_state=42)\n",
        "    model.fit(X_scaled, y)\n",
        "\n",
        "    #clear_output(wait=True)\n",
        "    update_plot()\n",
        "    display(VBox([HBox([x_slider, y_slider]), HBox([label_dropdown, add_button])]))\n",
        "    print(f\"Added point at ({new_x:.2f}, {new_y:.2f}) with label {'Blue' if new_label == 0 else 'Red'}\")\n",
        "\n",
        "# Connect button to add_point function\n",
        "add_button.on_click(add_point)\n",
        "\n",
        "def on_button_clicked(b):\n",
        "        add_point()\n",
        "\n",
        "# Initial plot\n",
        "update_plot()\n",
        "\n",
        "# Display widgets\n",
        "display(VBox([HBox([x_slider, y_slider]), HBox([label_dropdown, add_button])]))"
      ],
      "metadata": {
        "id": "3ntrPRnNTrwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The background colors in the plot represent the model's prediction probabilities for each point in the 2D space. Here's what the colors symbolize:\n",
        "<br/>\n",
        "Dark Blue: Areas where the model is very confident (high probability) that points belong to the \"Blue\" class (label 0).\n",
        "\n",
        "Light Blue: Areas where the model predicts the \"Blue\" class, but with less confidence (lower probability).\n",
        "\n",
        "White or Very Light Colors: Areas near the decision boundary where the model is uncertain (probabilities close to 0.5 for both classes).\n",
        "\n",
        "Light Red: Areas where the model predicts the \"Red\" class, but with less confidence.\n",
        "\n",
        "Dark Red: Areas where the model is very confident that points belong to the \"Red\" class (label 1).\n",
        "\n",
        "The color gradient from blue to red represents the continuous probability output of the neural network:\n",
        "\n",
        "0 (Dark Blue) → 0.5 (White/Very Light) → 1 (Dark Red)\n",
        "\n",
        "This visualization helps to show:\n",
        "\n",
        "The decision boundary: Where the colors transition from blue to red (often visible as a white or very light colored region).\n",
        "The model's confidence: Darker colors indicate higher confidence in the prediction for that region.\n",
        "Areas of uncertainty: Lighter colors, especially near the boundary between blue and red, show where the model is less certain.\n",
        "\n",
        "When you add new points, you should see these color regions shift, demonstrating how the model's predictions change across the entire space due to the new data."
      ],
      "metadata": {
        "id": "OaPNt-PnyD0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Membership Inference Attack (Model Stealing)\n",
        "\n",
        "This example shows how you can clone a model by querying the targeted model many times to get your training data from the targeted model and use that to train the cloned model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CrfYWDQvQvZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.hidden(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "# Create and train the \"black-box\" model\n",
        "black_box_model = SimpleNN(2, 10, 2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(black_box_model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    outputs = black_box_model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "black_box_model.eval()\n",
        "\n",
        "# Function to query the black-box model\n",
        "def query_black_box(X):\n",
        "    with torch.no_grad():\n",
        "        outputs = black_box_model(torch.FloatTensor(X))\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "    return predicted.numpy()\n",
        "\n",
        "# Generate a dataset for the attack by querying the black-box model\n",
        "# This simulates querying an API for a model and getting the label with different input feature values\n",
        "X_attack = np.random.rand(5000, 2) * 4 - 2  # Generate random points in the feature space\n",
        "y_attack = query_black_box(X_attack)\n",
        "\n",
        "# Convert attack data to PyTorch tensors\n",
        "X_attack_tensor = torch.FloatTensor(X_attack)\n",
        "y_attack_tensor = torch.LongTensor(y_attack)\n",
        "\n",
        "# Create and train the \"attack\" model\n",
        "attack_model = SimpleNN(2, 10, 2)\n",
        "attack_optimizer = optim.Adam(attack_model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    outputs = attack_model(X_attack_tensor)\n",
        "    loss = criterion(outputs, y_attack_tensor)\n",
        "    attack_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    attack_optimizer.step()\n",
        "\n",
        "attack_model.eval()\n",
        "\n",
        "# Evaluate both models\n",
        "def evaluate_model(model, X, y):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = (predicted == y).float().mean()\n",
        "    return accuracy.item()\n",
        "\n",
        "print(f\"Black-box model accuracy: {evaluate_model(black_box_model, X_test_tensor, y_test_tensor):.4f}\")\n",
        "print(f\"Attack model accuracy: {evaluate_model(attack_model, X_test_tensor, y_test_tensor):.4f}\")\n",
        "\n",
        "# Visualize the results\n",
        "def plot_decision_boundary(ax, model, X, y, title):\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
        "    Z = query_black_box(np.c_[xx.ravel(), yy.ravel()]) if model == black_box_model else model(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])).argmax(1).detach().numpy()\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolor='black')\n",
        "    ax.set_title(title)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "plot_decision_boundary(ax1, black_box_model, X_test, y_test, \"Original Black-box Model\")\n",
        "plot_decision_boundary(ax2, attack_model, X_test, y_test, \"Extracted Attack Model\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PKuMvzfpxkpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see here you were able to create a copy of the target model that has very similar decision boundaries."
      ],
      "metadata": {
        "id": "ZN7it4TNSYbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Set Data Leakage\n",
        "\n"
      ],
      "metadata": {
        "id": "YRwiTAoVUDXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Dataset Leakage/Memorization in Neural Networks\n",
        "\n",
        "Training dataset leakage, also known as data memorization, occurs when a neural network learns to recognize specific examples from its training set rather than generalizing patterns. This can lead to abnormally high confidence levels for inputs similar to or identical to those in the training set, while performing poorly on truly novel data.\n",
        "<br/><br/>\n",
        "\n",
        "Causes:\n",
        "<br/>\n",
        "\n",
        "**Overtraining:** Training the model for too many epochs can cause it to memorize the training data instead of learning general patterns.\n",
        "\n",
        "**Long-tail distribution: When the training data contains rare examples, the model may memorize these instances rather than learning to generalize.\n",
        "\n",
        "**Insufficient regularization:** Lack of proper regularization techniques can allow the model to fit noise in the training data.\n",
        "\n",
        "**Limited dataset size:** Smaller datasets increase the risk of memorization as the model has fewer examples to learn from.\n",
        "\n",
        "**High model capacity:** Models with excessive parameters relative to the dataset size are more prone to memorization.\n",
        "\n",
        "Here's a sample code that demonstrates how to identify inputs that correlate to training data by examining confidence levels:"
      ],
      "metadata": {
        "id": "ZLAuYZZwlwC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a simple dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(1000, 2)  # 1000 points with 2 features\n",
        "y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Simple rule: sum of features > 1\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train}\")\n",
        "\n",
        "\n",
        "# Create a model prone to memorization\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(2,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model for many epochs to encourage memorization\n",
        "history = model.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=0)\n",
        "\n",
        "# Function to plot predictions\n",
        "def plot_predictions(X, y, title):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', alpha=0.7)\n",
        "    plt.colorbar(label='Prediction Confidence')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.show()\n",
        "\n",
        "# Predict on training and test data\n",
        "train_pred = model.predict(X_train).flatten()\n",
        "test_pred = model.predict(X_test).flatten()\n",
        "\n",
        "# Plot predictions\n",
        "plot_predictions(X_train, train_pred, \"Training Data Predictions\")\n",
        "plot_predictions(X_test, test_pred, \"Test Data Predictions\")\n",
        "\n",
        "# Print some statistics\n",
        "print(f\"Average confidence on training data: {np.mean(train_pred):.4f}\")\n",
        "print(f\"Average confidence on test data: {np.mean(test_pred):.4f}\")\n",
        "print(f\"Percentage of high-confidence (>0.9) predictions on training data: {np.mean(train_pred > 0.9):.2%}\")\n",
        "print(f\"Percentage of high-confidence (>0.9) predictions on test data: {np.mean(test_pred > 0.9):.2%}\")\n",
        "\n",
        "# Function to test user inputs\n",
        "def test_user_input():\n",
        "    while True:\n",
        "        try:\n",
        "            x1 = float(input(\"Enter value for feature 1 (between 0 and 1): \"))\n",
        "            x2 = float(input(\"Enter value for feature 2 (between 0 and 1): \"))\n",
        "            if 0 <= x1 <= 1 and 0 <= x2 <= 1:\n",
        "                break\n",
        "            else:\n",
        "                print(\"Please enter values between 0 and 1.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter numeric values.\")\n",
        "\n",
        "    user_input = np.array([[x1, x2]])\n",
        "    prediction = model.predict(user_input)[0, 0]\n",
        "    print(f\"Model prediction: {prediction:.4f}\")\n",
        "    print(f\"Actual class: {int(x1 + x2 > 1)}\")\n",
        "\n",
        "# Test user inputs\n",
        "print(\"\\nTest your own inputs:\")\n",
        "for _ in range(3):\n",
        "    test_user_input()\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "AQAg87zKSgHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "imaPIzhlrDiK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}